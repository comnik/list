\documentclass[a4paper, 11pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[pdftex]{hyperref}

% Lengths and indenting
\setlength{\textwidth}{16.5cm}
\setlength{\marginparwidth}{1.5cm}
\setlength{\parindent}{0cm}
\setlength{\parskip}{0.15cm}
\setlength{\textheight}{22cm}
\setlength{\oddsidemargin}{0cm}
\setlength{\evensidemargin}{\oddsidemargin}
\setlength{\topmargin}{0cm}
\setlength{\headheight}{0cm}
\setlength{\headsep}{0cm}

\renewcommand{\familydefault}{\sfdefault}

\title{Introduction to Learning and Intelligent Systems - Spring 2015}
\author{etienned@ethz.ch\\ ary@student.ethz.ch\\ ngoebel@student.ethz.ch\\}
\date{\today}

\begin{document}
\maketitle

\section*{Project 1 : Regression}

\subsection*{Preparation}

We establish a simple pipeline for transforming the input file into our feature-matrix.
This is encapsulated in the function \texttt{get\_features}.
It takes a path as an input, reads the csv row-wise and
transforms those rows into feature-vectors, using the \texttt{to\_feature\_vec} function.

We also transform the Y-values to $\log(1 + Y)$, thus reducing the problem
to a vanilla least-squares regression.

\subsection*{Feature Extraction}

By

\subsection*{Training}

We settled on a random forest regressor, to combat underfitting errors in
our previous approaches (linear regressor, ridge regressor). Other approaches
included Support-Vector Regression with gaussian-, linear- and polynomial kernels,
but we failed in achieving scores below 0.8 and had to abort grid-search approaches
for parameter estimation after multiple hours of runtime.

Another renegade approach was K-Nearest-Neighbour-Regression, which we were able
to fine tune up until a score of 0.43. Following our success with the random forest regressor, we regretfully
abandoned this line of enquiry.

Finally we experimented with different numbers of estimators for the random forest regressor,
settling on 30.


\end{document}
