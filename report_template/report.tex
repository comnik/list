\documentclass[a4paper, 11pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[pdftex]{hyperref}

% Lengths and indenting
\setlength{\textwidth}{16.5cm}
\setlength{\marginparwidth}{1.5cm}
\setlength{\parindent}{0cm}
\setlength{\parskip}{0.15cm}
\setlength{\textheight}{22cm}
\setlength{\oddsidemargin}{0cm}
\setlength{\evensidemargin}{\oddsidemargin}
\setlength{\topmargin}{0cm}
\setlength{\headheight}{0cm}
\setlength{\headsep}{0cm}

\renewcommand{\familydefault}{\sfdefault}

\title{Introduction to Learning and Intelligent Systems - Spring 2015}
\author{etienned@ethz.ch\\ ary@student.ethz.ch\\ ngoebel@student.ethz.ch\\}
\date{\today}

\begin{document}
\maketitle

\section*{Project 1 : Regression}

\subsection*{Preparation}

We establish a simple pipeline for transforming the input file into our feature-matrix.
This is encapsulated in the function \texttt{get\_features}.
It takes a path as an input, reads the csv row-wise and
transforms those rows into feature-vectors, using the \texttt{to\_feature\_vec} function.

We also transform the Y-values to $\log(1 + Y)$, thus reducing the problem
to a vanilla least-squares regression.

\subsection*{Feature Extraction}

By plotting the inputs in various ways we noticed, as expected,
a major periodicity with respect to time. Our first try in making use
of this knowledge consisted of adding trigonometric features of the
hour of the day, as well as under multipler periods, to simulate something
resembling a Fourier-decomposition. We later settled for a simpler approach
of categorizing all inputs by day of the year, hour of the day, as well as day of the week,
using one-hot vectors of the respective length.

We tried simulating polynomial features using a kernelized regressor, but either
failed to achieve a promising score or sensible training times. Thus we
had to use \texttt{scikit.preprocessing.PolynomialFeatures} to generate polynomial
features of up to degree three. Higher degrees did not seem to improve the score meaningfully.


\subsection*{Training}

We settled on a Random Forest regressor, to combat underfitting errors in
our previous approaches (linear regressor, ridge regressor). Other approaches
included Support-Vector Regression with gaussian-, linear- and polynomial kernels,
but we failed in achieving scores below 0.8 and had to abort grid-search approaches
for parameter estimation after multiple hours of runtime.

Another renegade approach was K-Nearest-Neighbour-Regression, which we were able
to fine tune up until a score of 0.43. Following our success with the Random Forest regressor, we regretfully
abandoned this line of enquiry.

Finally we experimented with different numbers of estimators for the Random Forest regressor,
settling on 30.


\end{document}
